{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mJQGbTFOCAx"
   },
   "source": [
    "<h1 align='center'><u>Spam Detection</u></h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 4192,
     "status": "ok",
     "timestamp": 1633182572739,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "Z8KpPS9Cl9tD",
    "outputId": "1864ef81-71c7-4615-a770-1f320ae56650"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 3264,
     "status": "ok",
     "timestamp": 1633182575997,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "_fSmnBkyCZd5",
    "outputId": "514c37ef-31da-4876-e406-5c1c0c411857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.7/dist-packages (0.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 3375,
     "status": "ok",
     "timestamp": 1633182579368,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "lleWVrW2fJRB",
    "outputId": "229b2896-f04e-46a0-e070-8d4120b10cde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4573,
     "status": "ok",
     "timestamp": 1633182583927,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "2vzoDb7CltyX"
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import textwrap as tw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# learning Curves\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# save and load models\n",
    "import joblib\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Token\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1633182583932,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "GsVn1VqQmQGA",
    "outputId": "19e911d0-117a-454e-b2b2-848e32584f1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1633182583938,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "e9Vsbbb5mXnr",
    "outputId": "ef29f1c5-5e8e-4015-b0a5-a06144742d62"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'3.1.3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1633182583939,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "wMInbDePmsRC"
   },
   "outputs": [],
   "source": [
    "data_folder = Path('/content/drive/MyDrive/Data/Spam_detection/')\n",
    "spacy_folder = Path('/content/drive/MyDrive/Data/spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 864,
     "status": "ok",
     "timestamp": 1633182584788,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "0Y2uAA66m_DD"
   },
   "outputs": [],
   "source": [
    "# load spacy model\n",
    "model = spacy_folder /'en_core_web_sm-3.1.0'/'en_core_web_sm'/'en_core_web_sm-3.1.0'\n",
    "nlp = spacy.load(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXeG6ZQ4OVDj"
   },
   "source": [
    "# Load the dataset \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1633182584793,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "WIXm9xz8kBQd"
   },
   "outputs": [],
   "source": [
    "# location of data file\n",
    "spam_file = data_folder / 'spam.csv'\n",
    "\n",
    "# creating Pandas Dataframe\n",
    "spam = pd.read_csv(spam_file, index_col=0,encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1633182584793,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "kiBawPgdqeNN",
    "outputId": "2cc31086-10d0-49b9-f7d6-57a979e4104e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data set is : (5572, 4)\n"
     ]
    }
   ],
   "source": [
    "# print shape of the dataset\n",
    "print(f'Shape of data set is : {spam.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1633182584794,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "lp_Ne2NYwkti",
    "outputId": "2f67bf30-0f2b-401e-93ca-70a79eab87e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   label    5572 non-null   object\n",
      " 1   message  5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "spam.drop(columns = ['Unnamed: 2','Unnamed: 3', 'Unnamed: 4'], inplace=True)\n",
    "spam.reset_index(inplace=True)\n",
    "spam.rename(columns={'v1':'label', 'v2':'message'},inplace=True)\n",
    "\n",
    "# Printing basic info\n",
    "spam.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1633182585000,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "sa0EazNurudw",
    "outputId": "eecaabd9-7001-41ee-8d90-ed3627bef24b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking distribution of class labels for train dataset\n",
    "spam['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1633182585001,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "pFgCwo3lyqbk"
   },
   "outputs": [],
   "source": [
    "save_model_folder = \"/content/drive/MyDrive/Colab Notebooks/NLP/saved_models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNdWCViagO4F"
   },
   "source": [
    "# Metric for evaluating model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1633182585001,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "dTjftr2jr3-F",
    "outputId": "bf88241b-f419-4992-dd50-3d9fb17ffb8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_scorer(fbeta_score, beta=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a scorer for F2 score so that we can given an emphases on the minority class predictions i.e higher recall\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "f2score = make_scorer(fbeta_score, beta=2)\n",
    "f2score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3VxdQtNsWzD"
   },
   "source": [
    "Using accuracy as the metric is not optimal to making the best prediction especially due to the fact that our data set is imbalanced\n",
    "\n",
    "To achieve an optimal result we would like to maximize two components:\n",
    "\n",
    "(1) The \"True Positive Rate\" aka Sensitivity aka Recall. Given by: TP/(TP+FN)\n",
    "\n",
    "(2) The Precision - How many of the positive predictions, are in fact correct. Given by: TP/(TP+FP)\n",
    "\n",
    "To obtain a balance between both we use F Beta Measure which is given by: (2 x Precision x Recall)/(Precision+Recall)\n",
    "F2-measure puts more attention on increasing reacall and minimizing false negatives which is critical for our problem statement given that we would not want to miss any actual spam messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rH8_mvGhiThV"
   },
   "source": [
    "# Classification Pipeline\n",
    "\n",
    "* You will now use all these technique and create following  classification pipelines\n",
    "\n",
    "    1. Data Preprocessing + Sparse Embeddings (TF-IDF) + ML Model pipeline\n",
    "    2. Feature Engineering + ML Model pipeline\n",
    "    3. Featurization (TF-IDF) + Feature Engineering + ML Model pipeline\n",
    "\n",
    "**Approach:**\n",
    "\n",
    "**Using a smaller subset of dataset (recommended 40 %) to evaluate the three pipelines . Based on your analysis (e.g. model score, learning curves) , choose one pipeline from the three. Train only the final pipeline on complete data.**\n",
    "\n",
    "**Pipeline Requirements:** \n",
    "\n",
    "1. Using XgBoost model for the classification. Tuning the **XGBoost for imbalanced dataset**: https://machinelearningmastery.com/xgboost-for-imbalanced-classification/).\n",
    "\n",
    "2. For feature engineering, creating following features:Count of following  (Nouns, ProperNouns, AUX, VERBS, Adjectives, named entities, spelling mistakes (see the link on how to get spelling mistakes https://pypi.org/project/pyspellchecker/). \n",
    "\n",
    "3. For Sparse embeddings you using **tfidf vectorization**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HO-ifjW0q0d"
   },
   "source": [
    "## Sampling and Train-Test Split\n",
    "\n",
    "Using complete datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1633182585001,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "QIJZrWEDr9aZ"
   },
   "outputs": [],
   "source": [
    "spam['label'] = spam['label'].map({'spam':1, 'ham':0}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1633182585002,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "Knl_ZSHgvux4",
    "outputId": "5922c77f-8acb-4460-de0d-dc1e7c6ca956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (4179,) y_train: (4179,)\n",
      "X_test: (1393,) y_test: (1393,)\n"
     ]
    }
   ],
   "source": [
    "X = spam['message'].values\n",
    "y = spam['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "print(f'X_train: {X_train.shape} y_train: {y_train.shape}')\n",
    "print(f'X_test: {X_test.shape} y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qi9CVtJ1XycF"
   },
   "source": [
    "## Custom Classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-12T21:55:00.935471Z",
     "iopub.status.busy": "2021-09-12T21:55:00.935062Z",
     "iopub.status.idle": "2021-09-12T21:55:00.961087Z",
     "shell.execute_reply": "2021-09-12T21:55:00.960246Z",
     "shell.execute_reply.started": "2021-09-12T21:55:00.935426Z"
    },
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1633182585202,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "kqQR54ps8K42",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessor Function\n",
    "class SpacyPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    np.random.seed(0)\n",
    "    def __init__(self, lammetize=True, lower=True, remove_stop=True, \n",
    "                 remove_punct=True, remove_email=True, remove_url=True, remove_num=False, stemming = False,\n",
    "                 add_user_mention_prefix=True, remove_hashtag_prefix=False):\n",
    "        self.remove_stop = remove_stop\n",
    "        self.remove_punct = remove_punct\n",
    "        self.remove_num = remove_num\n",
    "        self.remove_url = remove_url\n",
    "        self.remove_email = remove_email\n",
    "        self.lammetize = lammetize\n",
    "        self.lower = lower\n",
    "        self.stemming = stemming\n",
    "        self.add_user_mention_prefix = add_user_mention_prefix\n",
    "        self.remove_hashtag_prefix = remove_hashtag_prefix\n",
    "\n",
    " # helpfer functions for basic cleaning \n",
    "\n",
    "    def basic_clean(self, text):\n",
    "        \n",
    "        '''\n",
    "        This fuction removes HTML tags from text\n",
    "        '''\n",
    "        if (bool(BeautifulSoup(text, \"html.parser\").find())==True):         \n",
    "            soup = BeautifulSoup(text, \"html.parser\")\n",
    "            text = soup.get_text()\n",
    "        else:\n",
    "            pass\n",
    "        return re.sub(r'[\\n\\r]',' ', text) \n",
    "\n",
    "    # helper function for pre-processing with spacy and Porter Stemmer\n",
    "    \n",
    "    def spacy_preprocessor(self,texts):\n",
    "\n",
    "        final_result = []\n",
    "        nlp = spacy.load(model, disable=['parser','ner'])\n",
    "        \n",
    "        ## Add @ as a prefix so that we can separate the word from its token\n",
    "        prefixes = list(nlp.Defaults.prefixes)\n",
    "\n",
    "        if self.add_user_mention_prefix:\n",
    "            prefixes += ['@']\n",
    "\n",
    "        ## Remove # as a prefix so that we can keep hashtags and words together\n",
    "        if self.remove_hashtag_prefix:\n",
    "            prefixes.remove(r'#')\n",
    "\n",
    "        prefix_regex = spacy.util.compile_prefix_regex(prefixes)\n",
    "        nlp.tokenizer.prefix_search = prefix_regex.search\n",
    "\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "        if self.remove_stop:\n",
    "            matcher.add(\"stop_words\", [[{\"is_stop\" : True}]])\n",
    "        if self.remove_punct:\n",
    "            matcher.add(\"punctuation\",[ [{\"is_punct\": True}]])\n",
    "        if self.remove_num:\n",
    "            matcher.add(\"numbers\", [[{\"like_num\": True}]])\n",
    "        if self.remove_url:\n",
    "            matcher.add(\"urls\", [[{\"like_url\": True}]])\n",
    "        if self.remove_email:\n",
    "            matcher.add(\"emails\", [[{\"like_email\": True}]])\n",
    "            \n",
    "        Token.set_extension('is_remove', default=False, force=True)\n",
    "\n",
    "        cleaned_text = []\n",
    "        for doc in nlp.pipe(texts,batch_size= 500,disable=['parser','ner'], n_process = 3):\n",
    "            matches = matcher(doc)\n",
    "            for _, start, end in matches:\n",
    "                for token in doc[start:end]:\n",
    "                    token._.is_remove =True\n",
    "                    \n",
    "            if self.lammetize:              \n",
    "                text = ' '.join(token.lemma_ for token in doc if (token._.is_remove==False))\n",
    "            elif self.stemming:\n",
    "                text = ' '.join(PorterStemmer().stem(token.text) for token in doc if (token._.is_remove==False))\n",
    "            else:\n",
    "                text = ' '.join(token.text for token in doc if (token._.is_remove==False))\n",
    "                                   \n",
    "            if self.lower:\n",
    "                text=text.lower()\n",
    "            cleaned_text.append(text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def fit(self, X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        try:\n",
    "            if str(type(X)) not in [\"<class 'list'>\",\"<class 'numpy.ndarray'>\"]:\n",
    "                raise Exception('Expected list or numpy array got {}'.format(type(X)))\n",
    "            x_clean = [self.basic_clean(text) for text in X]\n",
    "            x_clean_final = self.spacy_preprocessor(x_clean)\n",
    "            return x_clean_final\n",
    "        except Exception as error:\n",
    "            print('An exception occured: ' + repr(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-12T21:55:01.989305Z",
     "iopub.status.busy": "2021-09-12T21:55:01.989022Z",
     "iopub.status.idle": "2021-09-12T21:55:02.002533Z",
     "shell.execute_reply": "2021-09-12T21:55:02.001872Z",
     "shell.execute_reply.started": "2021-09-12T21:55:01.989275Z"
    },
    "executionInfo": {
     "elapsed": 1047,
     "status": "ok",
     "timestamp": 1633182586248,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "IsV7LP6F7iaO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    np.random.seed(0)\n",
    "    nlp = spacy.load(model, disable=['parser'])\n",
    "    spell = SpellChecker()\n",
    "    def __init__(self, word_count=False, char_count=False, char_count_wo_space=False, \n",
    "                 avg_word_length=False, digit_count=False, noun_count= True, propernoun_count=True, \n",
    "                 verb_count=True, aux_count= True, adj_count= True, ner_count= True, misspelled_count=True):\n",
    "        self.word_count = word_count\n",
    "        self.char_count = char_count\n",
    "        self.char_count_wo_space = char_count_wo_space\n",
    "        self.avg_word_length = avg_word_length\n",
    "        self.digit_count = digit_count\n",
    "        self.noun_count = noun_count\n",
    "        self.propernoun_count = propernoun_count\n",
    "        self.verb_count = verb_count\n",
    "        self.aux_count = aux_count\n",
    "        self.adj_count = adj_count\n",
    "        self.ner_count = ner_count\n",
    "        self.misspelled_count= misspelled_count\n",
    "  \n",
    "    def fit(self, X,y=None):\n",
    "        return self\n",
    "\n",
    "    #Useful functions\n",
    "\n",
    "    def wordCount(self,text):\n",
    "        return len(text.split())\n",
    "\n",
    "    def charCount(self,text):\n",
    "        return len(text)\n",
    "\n",
    "    def charCountWithoutSpace(self,text):\n",
    "        count = 0\n",
    "        for word in text.split():\n",
    "            count += len(word)\n",
    "        return count\n",
    "\n",
    "    def avgWordLength(self,text):\n",
    "        word_length = 0\n",
    "        for token in text.split():\n",
    "            word_length += len(token)\n",
    "        word_count = len(text.split())\n",
    "        if word_count == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return word_length/word_count\n",
    "\n",
    "    def digitCount(self,text):\n",
    "        count = 0\n",
    "        for i in text:\n",
    "            if i.isdigit():\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    \n",
    "\n",
    "    def nouncount(self, text):\n",
    "      doc = nlp(text)\n",
    "      noun_tokens = [token.text for token in doc if(token.pos_== 'NOUN')]        \n",
    "      return len(noun_tokens)\n",
    "\n",
    "    def propernouncount(self, text):\n",
    "      doc = nlp(text)\n",
    "      prnoun_tokens = [token.text for token in doc if(token.pos_== 'PROPN')]       \n",
    "      return len(prnoun_tokens)\n",
    "    \n",
    "    def verbcount(self, text):\n",
    "      doc = nlp(text)\n",
    "      verb_tokens = [token.text for token in doc if(token.pos_== 'VERB')]         \n",
    "      return len(verb_tokens)\n",
    "    \n",
    "    def auxcount(self, text):\n",
    "      doc = nlp(text)\n",
    "      aux_tokens = [token.text for token in doc if(token.pos_== 'AUX')]        \n",
    "      return len(aux_tokens)\n",
    "\n",
    "    def adjcount(self, text):\n",
    "      doc = nlp(text)\n",
    "      adj_tokens = [token.text for token in doc if(token.pos_== 'ADJ')]        \n",
    "      return len(adj_tokens)\n",
    "\n",
    "    def nercount(self, text):\n",
    "      doc = nlp(text)\n",
    "      ner = [entity.text for entity in doc.ents]       \n",
    "      return len(ner)\n",
    "\n",
    "    def misspelledcount(self,text):\n",
    "      doc = nlp(text)\n",
    "      tokens = [token.text for token in doc]\n",
    "      misspelled_tokens = SpellChecker().unknown(tokens)       \n",
    "      return len(misspelled_tokens)\n",
    "\n",
    "    def transform(self, X,y=None):\n",
    "        try:\n",
    "            if str(type(X)) not in [\"<class 'list'>\",\"<class 'numpy.ndarray'>\"]:\n",
    "                raise Exception('Expected list or numpy array got {}'.format(type(X)))\n",
    "            final_result = []\n",
    "            for index,item in enumerate(X):\n",
    "                res = []\n",
    "                if self.word_count:\n",
    "                    res.append(self.wordCount(item))\n",
    "                if self.char_count:\n",
    "                    res.append(self.charCount(item))\n",
    "                if self.char_count_wo_space:\n",
    "                    res.append(self.charCountWithoutSpace(item))\n",
    "                if self.avg_word_length:\n",
    "                    res.append(self.avgWordLength(item))\n",
    "                if self.digit_count:\n",
    "                    res.append(self.digitCount(item))\n",
    "                if self.noun_count:\n",
    "                    res.append(self.nouncount(item))\n",
    "                if self.propernoun_count:\n",
    "                    res.append(self.propernouncount(item))\n",
    "                if self.verb_count:\n",
    "                    res.append(self.verbcount(item))\n",
    "                if self.aux_count:\n",
    "                    res.append(self.auxcount(item))\n",
    "                if self.adj_count:\n",
    "                    res.append(self.adjcount(item))\n",
    "                if self.ner_count:\n",
    "                    res.append(self.nercount(item))\n",
    "                if self.misspelled_count:\n",
    "                    res.append(self.misspelledcount(item))\n",
    "                final_result.append(res)\n",
    "            return np.array(final_result)\n",
    "        except Exception as error:\n",
    "            print('An exception occured: ' + repr(error))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1633182586249,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "E8ht-OIqIZUs"
   },
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "  def fit(self, X, y=None, **fit_params):\n",
    "      return self\n",
    "\n",
    "  def transform(self, X, y=None, **fit_params):\n",
    "      return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T07:28:14.808298Z",
     "iopub.status.busy": "2021-07-29T07:28:14.808169Z",
     "iopub.status.idle": "2021-07-29T07:28:14.815256Z",
     "shell.execute_reply": "2021-07-29T07:28:14.814981Z",
     "shell.execute_reply.started": "2021-07-29T07:28:14.808285Z"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1633182586249,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "JgOesmtW-ac3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate 2 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        An estimator instance implementing `fit` and `predict` methods which\n",
    "        will be cloned for each validation.\n",
    "\n",
    "    title : str\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Training vector, where ``n_samples`` is the number of samples and\n",
    "        ``n_features`` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples) or (n_samples, n_features)\n",
    "        Target relative to ``X`` for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array-like of shape (3,), default=None\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple of shape (2,), default=None\n",
    "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like of shape (n_ticks,)\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the ``dtype`` is float, it is regarded\n",
    "        as a fraction of the maximum size of the training set (that is\n",
    "        determined by the selected validation method), i.e. it has to be within\n",
    "        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n",
    "        sets. Note that for classification the number of samples usually have\n",
    "        to be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       return_times=True,\n",
    "                       random_state=123)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BSiMky5rBFM"
   },
   "source": [
    "## Final Pipeline - Complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1633182586249,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "_KW-2aVctmuS",
    "outputId": "1b33ae62-9075-4512-be7d-6fad90724ee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate: 6.459\n"
     ]
    }
   ],
   "source": [
    "# count examples in each class\n",
    "counter = Counter(spam['label'])\n",
    "# estimate scale_pos_weight value\n",
    "estimate = counter[0] / counter[1]\n",
    "print('Estimate: %.3f' % estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1633182586250,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "-e6QH1PDrO5_"
   },
   "outputs": [],
   "source": [
    "vectorization = Pipeline([ ('vectorizer', TfidfVectorizer(analyzer='word', token_pattern=r\"[\\S]+\")),\n",
    "                         ])                        \n",
    "feature_engineering = FeatureEngineering()\n",
    "combined_features = FeatureUnion([(\"vec\", vectorization), (\"fe\", feature_engineering)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1633182586250,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "7NnujtyGrjAB"
   },
   "outputs": [],
   "source": [
    "classifier_3 = Pipeline([('cf', combined_features),\n",
    "                  ('classifier', XGBClassifier(scale_pos_weight=estimate)),\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1633182586250,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "9XM9bNvcrO19"
   },
   "outputs": [],
   "source": [
    "param_grid_classifier_3 = {'cf__vec__vectorizer__max_features': [100, 500],\n",
    "                           'cf__vec__vectorizer__max_df': [0.2, 0.6],\n",
    "                           'cf__vec__vectorizer__min_df': [0.01, 0.5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1633182586459,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "kJny8nUjuGuM"
   },
   "outputs": [],
   "source": [
    "grid_classifier_3 = GridSearchCV(estimator=classifier_3, param_grid=param_grid_classifier_3, cv = 2,scoring = f2score, n_jobs= -1, verbose = 4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 6192511,
     "status": "ok",
     "timestamp": 1633188778956,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "SF_D0xiDroBR",
    "outputId": "c690e3f1-026d-4203-f703-c456f72545af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed: 89.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('cf',\n",
       "                                        FeatureUnion(n_jobs=None,\n",
       "                                                     transformer_list=[('vec',\n",
       "                                                                        Pipeline(memory=None,\n",
       "                                                                                 steps=[('vectorizer',\n",
       "                                                                                         TfidfVectorizer(analyzer='word',\n",
       "                                                                                                         binary=False,\n",
       "                                                                                                         decode_error='strict',\n",
       "                                                                                                         dtype=<class 'numpy.float64'>,\n",
       "                                                                                                         encoding='utf-8',\n",
       "                                                                                                         input='content',\n",
       "                                                                                                         lowercase=True,\n",
       "                                                                                                         max_df=1.0,\n",
       "                                                                                                         max_features=None,\n",
       "                                                                                                         min_df=...\n",
       "                                                      scale_pos_weight=6.459170013386881,\n",
       "                                                      seed=None, silent=None,\n",
       "                                                      subsample=1,\n",
       "                                                      verbosity=1))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'cf__vec__vectorizer__max_df': [0.2, 0.6],\n",
       "                         'cf__vec__vectorizer__max_features': [100, 500],\n",
       "                         'cf__vec__vectorizer__min_df': [0.01, 0.5]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(fbeta_score, beta=2), verbose=4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_classifier_3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1633188778956,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "J0fbzl5TrocF",
    "outputId": "31f90952-62d7-4e88-d791-ed93e12dacb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.87\n",
      "Best parameters:  {'cf__vec__vectorizer__max_df': 0.6, 'cf__vec__vectorizer__max_features': 100, 'cf__vec__vectorizer__min_df': 0.01}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best cross-validation score: {:.2f}\".format(grid_classifier_3.best_score_))\n",
    "print(\"Best parameters: \", grid_classifier_3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 1121469,
     "status": "ok",
     "timestamp": 1633189900420,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "1PnNBfxcrqzU",
    "outputId": "b564810d-7a2a-45d3-a69c-ed6f2cffd9ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9344\n",
      "Test score: 0.8713\n"
     ]
    }
   ],
   "source": [
    "print('Train score: {:.4f}'.format(grid_classifier_3.score(X_train, y_train)))\n",
    "print('Test score: {:.4f}'.format(grid_classifier_3.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 284549,
     "status": "ok",
     "timestamp": 1633190184962,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "ttP9n5gcrqnk"
   },
   "outputs": [],
   "source": [
    "# predicted values for Test data set\n",
    "y_test_pred = grid_classifier_3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1633190184963,
     "user": {
      "displayName": "Neema Kunder",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgboQ1KdRP6HJ0JeosIOQGpfQ19w5qXBGAjcuEX=s64",
      "userId": "16796135583018709920"
     },
     "user_tz": 300
    },
    "id": "A5WDxRJZr7Db",
    "outputId": "f91b37cc-5475-4989-98c0-9476776e5ced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set classification report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      1196\n",
      "           1       0.79      0.89      0.84       197\n",
      "\n",
      "    accuracy                           0.95      1393\n",
      "   macro avg       0.89      0.93      0.91      1393\n",
      "weighted avg       0.96      0.95      0.95      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest set classification report:\\n\\n',classification_report(y_test, y_test_pred ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqPTPb0AsMP3"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NeemaBabu_file2_hw2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
